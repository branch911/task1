{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "(506, 12)\n",
      "(506,)\n",
      "Epoch [1/100], Loss: 324.9702\n",
      "Epoch [2/100], Loss: 339.4274\n",
      "Epoch [3/100], Loss: 413.8306\n",
      "Epoch [4/100], Loss: 235.2878\n",
      "Epoch [5/100], Loss: 321.8720\n",
      "Epoch [6/100], Loss: 69.2809\n",
      "Epoch [7/100], Loss: 80.9807\n",
      "Epoch [8/100], Loss: 296.1701\n",
      "Epoch [9/100], Loss: 25.4659\n",
      "Epoch [10/100], Loss: 32.2750\n",
      "Epoch [11/100], Loss: 8.7093\n",
      "Epoch [12/100], Loss: 42.3990\n",
      "Epoch [13/100], Loss: 32.9533\n",
      "Epoch [14/100], Loss: 38.9646\n",
      "Epoch [15/100], Loss: 15.2752\n",
      "Epoch [16/100], Loss: 33.0249\n",
      "Epoch [17/100], Loss: 32.2351\n",
      "Epoch [18/100], Loss: 32.3216\n",
      "Epoch [19/100], Loss: 16.5615\n",
      "Epoch [20/100], Loss: 6.0306\n",
      "Epoch [21/100], Loss: 16.3166\n",
      "Epoch [22/100], Loss: 7.2475\n",
      "Epoch [23/100], Loss: 30.0824\n",
      "Epoch [24/100], Loss: 24.5348\n",
      "Epoch [25/100], Loss: 12.6873\n",
      "Epoch [26/100], Loss: 10.7320\n",
      "Epoch [27/100], Loss: 24.2792\n",
      "Epoch [28/100], Loss: 8.9975\n",
      "Epoch [29/100], Loss: 31.0543\n",
      "Epoch [30/100], Loss: 12.0931\n",
      "Epoch [31/100], Loss: 4.5321\n",
      "Epoch [32/100], Loss: 15.9143\n",
      "Epoch [33/100], Loss: 23.4174\n",
      "Epoch [34/100], Loss: 25.6913\n",
      "Epoch [35/100], Loss: 10.2252\n",
      "Epoch [36/100], Loss: 9.1235\n",
      "Epoch [37/100], Loss: 15.2459\n",
      "Epoch [38/100], Loss: 16.1743\n",
      "Epoch [39/100], Loss: 13.2251\n",
      "Epoch [40/100], Loss: 78.5007\n",
      "Epoch [41/100], Loss: 38.5571\n",
      "Epoch [42/100], Loss: 22.1576\n",
      "Epoch [43/100], Loss: 10.3987\n",
      "Epoch [44/100], Loss: 16.3166\n",
      "Epoch [45/100], Loss: 13.0107\n",
      "Epoch [46/100], Loss: 7.3104\n",
      "Epoch [47/100], Loss: 24.3600\n",
      "Epoch [48/100], Loss: 16.8282\n",
      "Epoch [49/100], Loss: 5.3460\n",
      "Epoch [50/100], Loss: 8.6875\n",
      "Epoch [51/100], Loss: 3.5180\n",
      "Epoch [52/100], Loss: 20.2865\n",
      "Epoch [53/100], Loss: 38.9205\n",
      "Epoch [54/100], Loss: 8.4095\n",
      "Epoch [55/100], Loss: 5.7062\n",
      "Epoch [56/100], Loss: 6.7092\n",
      "Epoch [57/100], Loss: 12.4272\n",
      "Epoch [58/100], Loss: 12.7301\n",
      "Epoch [59/100], Loss: 33.0602\n",
      "Epoch [60/100], Loss: 10.1006\n",
      "Epoch [61/100], Loss: 43.3899\n",
      "Epoch [62/100], Loss: 15.1417\n",
      "Epoch [63/100], Loss: 67.2430\n",
      "Epoch [64/100], Loss: 21.4049\n",
      "Epoch [65/100], Loss: 27.8514\n",
      "Epoch [66/100], Loss: 19.3195\n",
      "Epoch [67/100], Loss: 22.6979\n",
      "Epoch [68/100], Loss: 58.9352\n",
      "Epoch [69/100], Loss: 15.7551\n",
      "Epoch [70/100], Loss: 12.0751\n",
      "Epoch [71/100], Loss: 22.6024\n",
      "Epoch [72/100], Loss: 8.6089\n",
      "Epoch [73/100], Loss: 5.3510\n",
      "Epoch [74/100], Loss: 27.3713\n",
      "Epoch [75/100], Loss: 35.3589\n",
      "Epoch [76/100], Loss: 18.0846\n",
      "Epoch [77/100], Loss: 10.0222\n",
      "Epoch [78/100], Loss: 12.4225\n",
      "Epoch [79/100], Loss: 21.6673\n",
      "Epoch [80/100], Loss: 6.6324\n",
      "Epoch [81/100], Loss: 10.0713\n",
      "Epoch [82/100], Loss: 34.7543\n",
      "Epoch [83/100], Loss: 135.3006\n",
      "Epoch [84/100], Loss: 7.1827\n",
      "Epoch [85/100], Loss: 11.1424\n",
      "Epoch [86/100], Loss: 20.6818\n",
      "Epoch [87/100], Loss: 19.3862\n",
      "Epoch [88/100], Loss: 10.3494\n",
      "Epoch [89/100], Loss: 12.9001\n",
      "Epoch [90/100], Loss: 56.3426\n",
      "Epoch [91/100], Loss: 12.7084\n",
      "Epoch [92/100], Loss: 19.0521\n",
      "Epoch [93/100], Loss: 16.8051\n",
      "Epoch [94/100], Loss: 25.2910\n",
      "Epoch [95/100], Loss: 33.7695\n",
      "Epoch [96/100], Loss: 22.6427\n",
      "Epoch [97/100], Loss: 5.0492\n",
      "Epoch [98/100], Loss: 21.4446\n",
      "Epoch [99/100], Loss: 11.5399\n",
      "Epoch [100/100], Loss: 13.9911\n",
      "Test Loss: 9.0501\n"
     ]
    }
   ],
   "source": [
    "#kaggleÈÇ£‰∏™Êï∞ÊçÆÈõÜ‰∏çÂ•ΩÁî®üò†ÔºåÈöè‰æøÂèàÊâæ‰∫Ü‰∏Ä‰∏™\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('/Users/branch/Desktop/boston_housing_data.csv')\n",
    "print(data.shape)\n",
    "\n",
    "# Â§ÑÁêÜ‰∏Ä‰∏ãËøô‰∏™Áæé‰∏ΩÁöÑÊï∞ÊçÆü§©\n",
    "data = data.interpolate(method = 'linear').fillna(data.mean())\n",
    "#Âà†Èô§‰∏Ä‰∏ãÊ≥¢Âä®ËæÉÂ§ßÁöÑÁ¨¨‰∫åÂàó\n",
    "data = data.drop(['ZN'], axis=1)\n",
    "\n",
    "\n",
    "x_data = data.iloc[:, :12].values\n",
    "y_data = data.iloc[:, 12].values\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "# Ê†áÂáÜÂåñ\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data = scaler.transform(x_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=56)\n",
    "train_x = torch.from_numpy(X_train.astype(np.float32))\n",
    "train_y = torch.from_numpy(y_train.astype(np.float32)).view(-1, 1)  \n",
    "test_x = torch.from_numpy(X_test.astype(np.float32))\n",
    "test_y = torch.from_numpy(y_test.astype(np.float32)).view(-1, 1)  \n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# ÂÆö‰πâ‰∏Ä‰∏ãÊ®°ÂûãüòÑ\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "#ÊçüÂ§±ÂáΩÊï∞Âíå‰ºòÂåñÂô®\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)\n",
    "\n",
    "# trainingÔºà0_0)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # ÂâçÂêë\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # ÂèçÂêë‰ºòÂåñ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# testÂπ∂ËØÑ‰º∞\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "    test_outputs = model(test_x)\n",
    "    test_loss = criterion(test_outputs, test_y)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
